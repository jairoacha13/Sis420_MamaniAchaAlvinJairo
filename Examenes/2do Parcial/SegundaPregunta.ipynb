{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4MgltYgZWM"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/024_mlp_clasificacion/mlp_clasificacion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NGidamugZWP"
      },
      "source": [
        "# El Perceptrón Multicapa - Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYquiZh4gZWQ"
      },
      "source": [
        "## El Perceptrón Multicapa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2Gqjm_gZWQ"
      },
      "source": [
        "En el [post](https://sensioai.com/blog/023_mlp_backprop) anterior hemos introducido el modelo de `Perceptrón Multicapa`, o MLP, la arquitectura de red neuronal más básica basada en el [Perceptrón](https://sensioai.com/blog/012_perceptron1). Hemos visto cómo calcular la salida de un MLP de dos capas a partir de unas entradas y cómo encontrar los pesos óptimos para una tarea de regresión. En este post vamos a mejorar la implementación de nuestro MLP de dos capas para que sea capaz también de llevar a cabo tareas de clasificación.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IyRgwesgZWR"
      },
      "source": [
        "## Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LPtbLsUgZWR"
      },
      "source": [
        "La mayoría del código que utilizaremos fue desarrollado para el `Perceptrón` y lo puedes encontrar en este [post](https://sensioai.com/blog/018_perceptron_final). Lo único que cambiaremos es la lógica del modelo, el resto de funcionalidad (funciones de pérdida, funciones de activación, etc, siguen siendo exactamente igual)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcsB8ZW6gZWS"
      },
      "source": [
        "### Funciones de activación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syl8VvjrgZWT"
      },
      "source": [
        "Para la capa oculta de nuestro MLP utilizaremos una función de activación de tipo `relu`, de la cual necesitaremos su derivada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MndmAqwzXLKU"
      },
      "outputs": [],
      "source": [
        "#---------------------------------------------------------------------------------\n",
        "# se utiliza para el manejo de rutas y directorios.\n",
        "import os\n",
        "\n",
        "# Calculo cientifico y vectorial para python\n",
        "import numpy as np\n",
        "\n",
        "# Librerias para graficar\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Modulo de optimización de scipy\n",
        "from scipy import optimize\n",
        "from scipy.io import loadmat\n",
        "# le dice a matplotlib que incruste gráficos en el cuaderno\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#Pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0WZ9f1f6jIMf"
      },
      "outputs": [],
      "source": [
        "def displayData(X, example_width=None, figsize=(10, 10)):\n",
        "    \"\"\"\n",
        "    Muestra datos 2D almacenados en X en una cuadrícula apropiada.\n",
        "    \"\"\"\n",
        "    # Calcula filas, columnas\n",
        "    if X.ndim == 2:\n",
        "        m, n = X.shape\n",
        "    elif X.ndim == 1:\n",
        "        n = X.size\n",
        "        m = 1\n",
        "        X = X[None]  # Promocionar a una matriz bidimensional\n",
        "    else:\n",
        "        raise IndexError('La entrada X debe ser 1 o 2 dimensinal.')\n",
        "\n",
        "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
        "    example_height = n / example_width\n",
        "\n",
        "    # Calcula el numero de elementos a mostrar\n",
        "    display_rows = int(np.floor(np.sqrt(m)))\n",
        "    display_cols = int(np.ceil(m / display_rows))\n",
        "\n",
        "    fig, ax_array = plt.subplots(display_rows, display_cols, figsize=figsize)\n",
        "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
        "\n",
        "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
        "\n",
        "    for i, ax in enumerate(ax_array):\n",
        "        ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n",
        "                  cmap='Greys', extent=[0, 1, 0, 1])\n",
        "        ax.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.041393Z",
          "start_time": "2020-08-05T16:49:53.024396Z"
        },
        "id": "HIq2-wCVgZWT"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j71DHXTEgZWU"
      },
      "source": [
        "En cuanto a las funciones de activación que utilizaremos a la salida del MLP, éstas son las que hemos introducido en posts anteriores:\n",
        "\n",
        "- Lineal: usada para regresión (junto a la función de pérdida MSE).\n",
        "- Sigmoid: usada para clasificación binaria (junto a la función de pérdida BCE).\n",
        "- Softmax: usada para clasificación multiclase (junto a la función de pérdida crossentropy, CE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.311543Z",
          "start_time": "2020-08-05T16:49:53.298548Z"
        },
        "id": "Yr2J98BBgZWV"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UStYvPKgZWV"
      },
      "source": [
        "### Funciones de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFQkqJthgZWW"
      },
      "source": [
        "Como acabamos de comentar en la sección anterior, estas son las funciones de pérdida que hemos visto hasta ahora para las diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.677416Z",
          "start_time": "2020-08-05T16:49:53.673343Z"
        },
        "id": "5jbCfdMMgZWW"
      },
      "outputs": [],
      "source": [
        "# Mean Square Error -> usada para regresión (con activación lineal)\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
        "def bce(y, y_hat):\n",
        "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
        "\n",
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c0_5lyNgZWX"
      },
      "source": [
        "Y sus derivadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.945375Z",
          "start_time": "2020-08-05T16:49:53.925371Z"
        },
        "id": "v5PbPP5wgZWX"
      },
      "outputs": [],
      "source": [
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_bce(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_crossentropy(y, y_hat):\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    answers[np.arange(len(y_hat)),y] = 1    \n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctn_7WsbgZWX"
      },
      "source": [
        "### Implementación MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M984aPUXgZWX"
      },
      "source": [
        "Ahora que ya tenemos definidas las diferentes funciones de activación y de pérdida que necesitamos, vamos a implementar nuestro MLP de dos capas capaz de llevar a cabo tanto tareas de regresión como de clasificación. Del mismo modo que ya hicimos con el `Perceptrón`, definiremos una clase base que servirá para la implementación de las clases particulares para cada caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:54.342062Z",
          "start_time": "2020-08-05T16:49:54.325063Z"
        },
        "code_folding": [
          3,
          14,
          20,
          55
        ],
        "id": "UwJ4CTGagZWY"
      },
      "outputs": [],
      "source": [
        "# clase base MLP \n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
        "    return self.activation(y_hat)\n",
        "    \n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):     \n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size] \n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x) \n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)        \n",
        "            # Backprop \n",
        "            dldy = self.grad_loss(y, y_pred) \n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:57:40.536617Z",
          "start_time": "2020-08-05T16:57:40.515590Z"
        },
        "id": "WQe4JD8qgZWY"
      },
      "outputs": [],
      "source": [
        "# MLP para regresión\n",
        "class MLPRegression(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
        "\n",
        "# MLP para clasificación binaria\n",
        "class MLPBinaryClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
        "\n",
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJZdpnoqgZWe"
      },
      "source": [
        "## Clasificación Multiclase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO8rfdK3gZWe"
      },
      "source": [
        "Por último vamos a ver cómo aplicar nuestro modelo para clasificación en multiples clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:05:47.527647Z",
          "start_time": "2020-08-05T17:05:47.393648Z"
        },
        "id": "JNxTRj79gZWe"
      },
      "outputs": [],
      "source": [
        "input_layer_size  = 400\n",
        "\n",
        "# 10 etiquetas, de 1 a 10 (tomar en cuenta que se asigna \"0\" a la etiqueta 10)\n",
        "num_labels = 10\n",
        "\n",
        "#  datos de entrenamiento almacenados en los arreglos X, y\n",
        "data = loadmat('ex3data1.mat')\n",
        "X, y = data['X'], data['y'].ravel()\n",
        "# establecer el dígito cero en 0, en lugar del 10 asignado a este conjunto de datos\n",
        "# Esto se hace debido a que el conjunto de datos se utilizó en MATLAB donde no hay índice 0\n",
        "y[y == 10] = 0\n",
        "salidas = len(np.unique(y))\n",
        "m, n = X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:08:01.961788Z",
          "start_time": "2020-08-05T17:08:01.891163Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD_jnMkogZWf",
        "outputId": "e31e4258-665b-4a6c-ac87-c836d98e2476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 500/1000, Loss: 0.66419\n",
            "Epoch: 1000/1000, Loss: 0.52544\n"
          ]
        }
      ],
      "source": [
        "model = MLPClassification(D_in=n, H=5, D_out=salidas)\n",
        "epochs, lr = 1000, 0.8\n",
        "model.fit(X, y, epochs, lr, batch_size=4899, log_each=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6YY1GDz3FdaM"
      },
      "outputs": [],
      "source": [
        "def interpretar(data):\n",
        "    listaMayor = []\n",
        "    for value in data:\n",
        "        mayor = 0\n",
        "        for index in range( len(value) ):\n",
        "            if( value[index] >= value[mayor]):\n",
        "                mayor = index\n",
        "        listaMayor.append(mayor)\n",
        "    return listaMayor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tXpJM4MFMOL",
        "outputId": "d1cf6ad0-286c-41f3-b2af-c6d1b99d3ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision del conjuto de entrenamiento: 80.00%\n"
          ]
        }
      ],
      "source": [
        "XPrueba = X[4899:4999, :].copy()\n",
        "peso = [model.w1, model.b1, model.w2, model.b2]\n",
        "prediccion = model.predict(peso, XPrueba)\n",
        "print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(interpretar(prediccion) == y[4899:4999]) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "p8F252Q_Y_OE",
        "outputId": "14550331-ae30-40b2-cd35-c2c75de4601b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El valor es:  8\n",
            "[[ -5.68989759   4.98696523   7.84617261   3.89747235  -5.77177134\n",
            "    5.63441758   5.16103341 -12.46601224  13.10691259   4.75939214]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "valor = 4267\n",
        "XPrueba = X[valor:valor+1, :].copy()\n",
        "print(\"El valor es: \", y[valor])\n",
        "peso = [model.w1, model.b1, model.w2, model.b2]\n",
        "prediccion = model.predict(peso, x=XPrueba)\n",
        "#numero2 = definirNumero(prediccion)\n",
        "print (prediccion)\n",
        "#print(\"El numero predicho es: \", numero2)\n",
        "displayData(X[valor:valor+1, :])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
